---
title: "Week 4: Predicting Student Achievement"
author: "Dr.Cansu Tatar"
format: html
editor: visual
---

# 1. Prepare

During the final week of each unit, we will complete a "case-study" to illustrate how Learning Analytics methods and techniques can be applied to address research questions of interest, create useful data products, and conduct reproducible research. Each case study is structured around a basic research workflow modeled after the Data Intensive Research Workflow from Learning Analytics Goes to School (Krumm, 2018).

Figure 1.

Steps of Data Intensive Research Workflow

![](Img/research-workflow.png){width="573"}

For Unit 1, we will focus on online science classes provided through a state-wide online virtual school and conduct an analysis that help predict students' performance in these online courses. This case study is guided by a foundational study in Learning Analytics that illustrates how analyses like these can be used develop an early warning system to help educators to identify students at risk of failing and intervene before that happens.

The Unit 1 case study will cover the following workflow topics.

1.  **Prepare:** Prior to analysis, it's critical to understand the context and data sources you're working with so you can formulate useful and answerable questions. You'll also need to become familiar with and load essential packages for analysis.
2.  **Wrangle:** Wrangling data entails the work of manipulating, cleaning, transforming, and merging data. In the part 2, we focus on importing CSV files, tidying and joining our data. For this assignment, we will only complete the half of the tidying process.
3.  **Explore:** In Part 3, we use basic data visualization and calculate some summary statistics to explore our data and see what insight it provides in response to our question.
4.  **Model:** After identifying variable that may be related to student performance through exploratory analysis, we'll look at correlations and create some simple models of our data using linear regression.
5.  **Communicate:** To wrap up our case study, we'll develop our first "data product" and share our analyses and findings by creating our first web page using R pubs.

## 1a. Review the Literature

Our Unit 1 Case Study is guided by a well-cited publication from two authors that have made numerous contributions to the field of Learning Analytics over the years. Although this article is focused on "early warning systems" in higher education, and where adoption of learning management systems (LMS) like Moodle and Canvas gained a quicker foothold, this study is particularly relevant since COVID-19. Many districts across the county have incorporated a LMS into their remote instruction and have set up virtual academies likely to continue post-pandemic.

**Note:** You have the following reading in your literature file. Check the full-text from there.

Macfadyen, L. P., & Dawson, S. (2010). [Mining LMS data to develop an "early warning system" for educators: A proof of concept.](https://www.sciencedirect.com/science/article/pii/S0360131509002486?via%3Dihub) *Computers & education*, *54*(2), 588-599

### Abstract

Earlier studies have suggested that higher education institutions could harness the predictive power of Learning Management System (LMS) data to develop reporting tools that identify at-risk students and allow for more timely pedagogical interventions. This paper confirms and extends this proposition by providing data from an international research project investigating **which student online activities accurately predict academic achievement.** Analysis of LMS tracking data from a Blackboard Vista-supported course identified 15 variables demonstrating a significant simple correlation with student final grade... Moreover, **network analysis of course discussion forums** afforded insight into the development of the student learning community by identifying disconnected students, patterns of student-to-student communication, and instructor positioning within the network. This study affirms that pedagogically meaningful information can be extracted from LMS-generated student tracking data, and discusses how these findings are informing the **development of a dashboard-like reporting tool for educators** that will extract and visualize real-time data on student engagement and likelihood of success.

### **Data Sources & Analysis**

The data analyzed in this exploratory research was extracted from the course-based instructor tracking logs and the BB VistaTM production server. Data collected on each student included 'whole term' counts for frequency of usage of course materials and tools supporting content delivery, engagement and discussion, assessment and administration/management. In addition, tracking data indicating total time spent on certain tool-based activities (assessments, assignments, total time online) offered a total measure of individual student time on task.

The authors used scatter plots for identifying potential correlational trends between variables under investigation, followed by a a simple correlation analysis of each variable to further interrogate the significance of selected variables as indicators of student achievement. Finally, a linear multiple regression analysis was therefore conducted, in order to develop a predictive model in which 'Student final grade' was the continuous dependent variable.

#### [**Your Turn**]{style="color: green;"} **⤵**

Take a quick scan of Table 1 in the article linked above (and also located in your "lit" folder in the files pane) and in the space below, answer the following question: Of the 13 LMS variables correlated with student final grade, which 2-3 do you think will found to significantly predict final grades?

-   Total Time Online, Time Spent on Assignments, and Time Spent on Assessments

Now take a quick look at Discussion and Conclusions sections and answer the following questions: What factors in the model did ultimately predict final grades? How accurate was this model in identifying "at risk" students?

-   number of forum postings, mail messages sent, and assessments completed. It seems like the model was somewhat accurate - the work showed that "some (but not all) LMS variables are useful predictors of student achievement in an LMS-supported course, but also that the predictive utility of many variables is dependent upon course site design and pedagogical goals."

## 1b. Define Questions

In this study, exploratory research was undertaken to identify the data variables that would inform the development of a data visualization tool for instructors. This involved the extraction of all LMS tracking variables for selected course sections at The University of British Columbia, Canada. In so doing, the study aimed to address the following research questions:

1.  Which LMS tracking data variables correlate significantly with student achievement?

2.  How accurately can measures of student online activity in an online course site predict student achievement in the course under study?

3.  Can tracking data recording online student communication patterns offer pedagogically meaningful insights into development of a student learning community?

For our case study, we'll adopt questions 1 & 2 wholesale to guide our exploratory analysis and modeling, with a special emphasis on time spent in the LMS. We'll also use analytical approaches and data similar to those used by \@macfadyen2010mining to better understand how LMS, gradebook, and survey data might be predictive of student performance.

## 1c. Load Libraries

As noted in our earlier class, R uses "packages" and add-ons that enhance its functionality. One of our first steps in any workflow is to load packages necessary for data wrangling, analysis, and reporting. We'll load the familiar {tidyverse} package in this section and introduce new packages and corresponding functions throughout the case study.

![](Img/tidyverse.png){width="354"}

Recall from earlier class that the {tidyverse} package is actually a [collection of R packages](https://www.tidyverse.org/packages) designed for reading, wrangling, and exploring data and which all share an underlying design philosophy, grammar, and data structures. These shared features are sometimes "tidy data principles."

Click the green arrow in the right corner of the "code chunk" that follows to load the {tidyverse} library introduced.

```{r}
library(tidyverse)
```

Don't worry if you saw a number of messages: those probably mean that the tidyverse loaded just fine. Any conflicts you may have seen mean that functions in these packages you loaded have the same name as functions in other packages and R will default to function from the last loaded package unless you specify otherwise.

# 2. Wrangle

In general, data wrangling involves some combination of cleaning, reshaping, transforming, and merging data [@wickham2023r]. The importance of data wrangling is difficult to overstate, as it involves the initial steps of going from raw data to a dataset that can be explored and modeled [@krumm2018] and often comprises a large share of time spent on data-intensive research workflows. In Part 2, we focus on the the following workflow processes:

a.  **Import Data**. In this section, we introduce the `read_csv()` function for working with CSV files and revisit some key functions for inspecting our data.

b.  **Tidy Data**. We introduce the `separate()` and `clean_names()` functions for getting our data nice and tidy, and revisit the `mutate()` function for creating new variables and feature engineering.

c.  **Join Data**. We conclude our data wrangling by introducing `join()` functions for merging our processed files into a single data frame for analysis.

### 2a. Import Data

Education data are stored in all sorts of different file formats and structures. In this course, we'll discuss several of these common formats, how to import your data into R, and how to transform you data into other data structures such as network objects required for social network analysis in Unit 3. In this case study, we'll focus on working with **comma-separated values (CSV)** files.

Similar to spreadsheet formats like Excel and Google Sheets, CSVs allow us to store rectangular data frames, but in a much simpler plain-text format, where all the important information in the file is represented by text. Note that "text" here refers to numbers, letters, and symbols you can type on your keyboard. In [Tidyverse Skills for Data Science](Tidyverse%20Skills%20for%20Data%20Science), Wright et al. @wright2021tidyverse note that the advantage of a .csv files is that:

> ... there are no workbooks or metadata making it difficult to open these files. CSVs are flexible files and are thus the preferred storage method for tabular data for many data scientists.

#### Data Source #1: Log Data

*Log-trace data* is data generated from our interactions with digital technologies, such as archived data from social media postings. In education, an increasingly common source of log-trace data is that generated from interactions with LMS and other digital learning tools.

The data we will use in this case study and already been "wrangled" quite a bit and is a summary of log-trace data: the number of minutes students spent on the course. While this data type is fairly straightforward, there are even more complex sources of log-trace data out there (e.g., time stamps associated with when students started and stopped accessing the course).

To help us import our data, we'll be using the {[readr](https://readr.tidyverse.org)} package, which provides a "fast and friendly way" to read rectangular data stored in plain-text file formats like csv, tsv, and fwf. If you are new to {readr}, I highly recommend the [data import chapter](https://r4ds.had.co.nz/data-import.html) in R for Data Science [@wickham2023r]. Note that we don't need to load the {readr} package because it was already loaded as part of the tidyverse package we called earlier.

Let's use the `read_csv()` function from {readr} to import our `log-data.csv` file directly from our data folder and name this data set `time_spent`, which will now be saved in our R environment:

```{r}
time_spent <- read_csv("Data/log-data.csv")
```

There are two quick things to note. First, importing our data and saving in our environment does not alter the original file saved in our data folder, so we don't have to worry about causing any harm to the original file as we wrangle and explore our data!

Second, the message in the output indicated that four of the columns were specified as "character" data types and two of the columns `student_id` and `time_spent` as double or numeric types.

#### [**Your Turn**]{style="color: green;"} **⤵**

Let's take a look at this data in a couple ways. First, type `time_spent` into the console (the pane below this one) and then hit return/enter. You should see a printed summary of this data frame.

Next, got your files pane (bottom right corner of R Studio), click on the data folder then click on the file named "log-data.csv" and in the pop-up click "View File."

What do you notice about this data? What questions do you have? Add a couple of notes (or more---you can type return/enter after a bullet point to add another) on your observations and/or questions here:

-   Many enrollment reasons pointed to courses not offered at local school.

#### Data Source #2: Academic Achievement Data

Academic achievement data is (obviously) is a very common form of data in education. In this case study, we'll use both the sum of the points students earned as well as the number of points possible to compute the percentage of points they earned in the course---a measure comparable (but likely a little different based on teachers' grading policies) to their final grade.

#### [**Your Turn**]{style="color: green;"} **⤵**

In the code chunk below, read in to R the `gradebook-summary.csv` file located in the data folder. You can use the code above as a template. Assign the output from the `read_csv()` function to a new object named gradebook.

```{r}
# YOUR CODE HERE

gradebook <- read_csv("Data/gradebook-summary.csv")

gradebook
```

#### Data Source #3: Self-Report Survey

The third data source is a self-report survey. This was data collected before the start of the course. The survey included ten items, each corresponding to one of three motivation measures: interest, utility value, and perceived competence. The three motivation measures we explore here come from Expectancy-Value Theory, which states that students are motivated to learn when they both believe that they can achieve something (expectancy, also known as "perceived competence") and believe that the concept they are trying to learn is important (value) [@wigfield2000expectancy]. For a more information about this survey, including the specific items included, see [Chapter 7 of Data Science in Education Using R](https://datascienceineducation.com/c07.html#data-sources) [@estrellado2020data].

#### [**Your Turn**]{style="color: green;"} **⤵**

In the code chunk below, read in to R the `survey.csv` file located in the data folder. You can use the code above as a template. Assign the output from the `read_csv()` function to a new object named `survey`.

```{r}
# YOUR CODE HERE

survey <- read_csv("Data/survey.csv")

survey
```

After reading the data, let's continue the practice of looking at our data. Type `survey` into the console to take a look at the data or add `survey` on a new line in your code chunk above. Does it appear to be the correct file? What do the variables seem to be about? What wrangling steps do we need to take? Taking a quick peak at the data helps us to begin to formulate answers to these questions is an important step in any data analysis.

Add one or more of the things you notice or wonder about the data here:

-   Most students report perceived competence.

**RStudio Tip:** If you happen to run into issues with data import, RStudio has a handy "Import Dataset" feature for a point and click approach to adding data to your environment. If you want to give this a try, be sure to pay attention to the default settings and the name it will give your data frame when imported.

![](Img/import.png){width="559"}

#### View Data

Once your data is in R, there are many different ways you can view it. Give each of the following at try:

```{r}
# enter the name of your data frame and view directly in the console or a code chunk
survey
```

```{r}
# view your data frame transposed so your can see every column and the first few entries
glimpse(survey) 
```

```{r}
# look at just the first six entries
head(survey) 
```

```{r}
# or the last six entries
tail(survey) 
```

```{r}
# view the names of your variables or columns
names(survey) 
```

```{r}
#| eval: false
# or view in source pane
View(survey)
```

Yes, the "V" is capitalized---very unusual for an R function (the tidyverse does have it's own lowercase `view()` function if that bothers you though). Because this function is a bit atypical in more ways than one, I have two recommendations concerning its use:

-   Use it strictly in the console. Because it opens a new viewing window, **including it in an R Markdown or Quarto document can cause issues when "knitting" or "rendering"** an HTML (or PDF) file. Hence I have included the `eval: false` option in the code chunk so it does not run when you render your document.

-   Close the viewer window that opens once you have viewed the data. Keeping it open can clutter your work space a bit and can lead to confusion about what data frame it was you viewed.

### 2b. Tidy Data

Tidy data refers to a consistent way to organize your data in R. In our course text, [R for Data Science](https://r4ds.had.co.nz/tidy-data.html), Hadley Wickham notes that getting your data into this format requires some work, but that work pays off in the long term:

> Once you have tidy data and the tidy tools provided by packages in the tidyverse, you will spend much less time munging data from one representation to another, allowing you to spend more time on the analytic questions at hand.

Not surprisingly, the Tidyverse set of packages including packages like `dplyr` adhere "tidy" data principles. Tidy data has three interrelated rules which make a dataset tidy:

1.  Each variable is a column

2.  Each observation is a row

3.  Each type of observational unit is a table

#### [**Your Turn**]{style="color: green;"} **⤵**

Why would this data be considered "untidy"? **Hint**: Look at the column names.

-   The column names don't give any indication what the variable is.

![](Img/untidy-data.png){width="616"}

#### Process Log Data

Earlier, we imported `time_spent`, which contains information on the number of minutes that students spent on the course, as well as other variables, particularly `course_id`.

Information about the course subject, semester, and section are not stored in a "tidy" format, but rather a single variable---`course_id`. This format of data storage is not ideal. If we instead give each piece of information its own column, we'll have more opportunities for later analysis. We'll use a function called `separate()` to do this.

First, let's practice with a small data set. We'll create it directly in R; run the code below to do that (and to assign the name `df` to the dataset).

```{r}
df <- tibble(course_var = c("Fall - Chemistry", 
                            "Fall - Earth Science", 
                            "Spring - Forensic Science",
                            "Spring - Earth Science",
                            "Spring - Biology"))

df
```

Print `df` to the console. You should see a single variable, `course_var`, with four rows.

In this (very small) data frame, there is information about both the semester and the course are encoded within the same variable. The `separate()` function has two primary arguments, one each for:

1.  the variable you want to separate
2.  the names of the new variables to create

Below, see using `course_var` for #1, and `c("Semester", "Course")` for #2, can be used to separate the semester and course data into two separate variable

```{r}
df |> 
  separate(course_var, c("semester", "course"))
```

Note that we used the `|>` operator called a **pipe** that was introduced in our earlier class. Recall that pipes, including the original `%>%` pipe operator, are a powerful tool for combining a sequence of functions or processes. Here we used it to send our `df` data frame to the `seprate()` function.

We could also have written the function as follows, with our data frame as the first argument in the function:

```{r}
separate(df, course_var, c("semester", "course"))
```

All of the {[dplyr](https://dplyr.tidyverse.org)} functions like and many {tidyverse} functions like `separate()` take a data frame (or [tibble](https://tibble.tidyverse.org)) as the first argument. However, it can be helpful for just for general code readability to separate (no pun intended) your data frame from the functions and starting your first line of code with the data frame to be wrangled. This is a pretty standard convention when writing R code.

Next, let's try something slightly different. Here, we have a data frame with a variable that encodes three pieces of information within the same variable: the year, semester, and subject. There are also a few other differences.

```{r}
df2 <- tibble(course_variable = c("19-Fall-Algebra1", 
                                  "20-Fall-Algebra2", 
                                  "20-Spring-Algebra2",
                                  "20-Spring-Algebra2",
                                  "21-Fall-Algebra1"))
df2
```

#### [**Your Turn**]{style="color: green;"} **⤵**

Can you separate the variable in the above data frame not into two, but rather three, new variables?

```{r}
# YOUR CODE HERE

separate(df, course_var, c("semester", "course", "time_spent"))
```

**Hint**: Try to modify the code from above (in which you separated `course_var` into two variables) based on a) the name of the variable in `df2` and b) adding the name for the third new variable you wish to create.

Let's return back to our `time_spent` data frame, now. It is often helpful to take a look at the data before writing code.

Below, we will load `time_spent` and run the `separate()` function with the `course_id` variable to split up the subject, semester, and section so we can use them later on. In other words, whereas above we separated the variable `course_variable`, in the data set we'll use here, we'll separate the `course_id` variable.

```{r}
time_spent |>
  separate(course_id,
           c("subject", "semester", "section"))
```

#### [**Your Turn**]{style="color: green;"} **⤵**

There is one last key step -- one that is likely to be a bit disorienting at first. Once we've processed the data how we would like, we have to assign, or save, the results back to the name for the data with which we have been working. This is done with the assignment operator, or the \<- symbol.

Copy the code you successfully ran in the chunk above to follow the assignment operator in the chunk below. In other words, write the code you wrote above, but assign the output back to `time_spent` so that it now includes the course id separated into three distinct variables.

```{r}
# YOUR CODE HERE
time_spent |>
  separate(time_spent,
           c("subject", "semester", "section"))

```

**Note**: It would have been perfectly acceptable, and in some cases preferable, to assign to a new name such as `time_spent_tidy`. However, to help keep our environment pane from becoming too cluttered and avoid confusion, we will reuse the same object name.

We have made a habit of continually looking at our data after running code to ensure that the step worked as intended. Type the name of the data we have been working with in the code chunk below to ensure that the course_id variable has been separated into three variables that correspond to the subject, semester, and section.

```{r}
# YOUR CODE HERE
time_spent

```

If something doesn't look right, consider re-running the code chunks above, perhaps returning all the way to the first code chunk that you ran (to load the data) to ensure that the output is as you intended for it to be.

# Next Class Time

We will finish the step 2: wrangle process together in our R tutorial series.

-   "Mutate" a column to change the time spent variable to represent hours

-   Process Gradebook Data

-   Process Survey Data

-   Joining the data

-   Change Data Type
